---
layout: article
title: 2019-02-15 Spark 集群配置
tags: [spark,Java]
key: e77f3828-2cd8-47cc-a062-9654b7ae095e
---

在 MAC 下使用虚拟机配置 Spark 集群.

<!--more-->

### 依赖版本

1. java1.8.0_161
2. scala 2.12.2
3. hadoop 2.8.3
4. spark 2.2.1

### virtualbox 虚拟机

安装三个虚拟机: hserver1(192.168.119.128), hserver2(192.168.119.129), hserver3(192.168.119.130), 使用 centos7 64 位系统.

虚拟机的网卡1设置为内网模式, 同时 hserver1 的网卡2使用桥接模式, 其公网ip为: 192.168.25.151

设置 hserver1 内网静态ip:

```
vi /etc/sysconfig/network-scripts/ifcfg-enp0s3

TYPE=Ethernet
BOOTPROTO=static
IPADDR=192.168.119.128
NETMASK=255.255.255.0
NM_CONTROLLED=no
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=enp0s3
UUID=200de1be-cba3-43a9-babd-d7eb0448c6e4
DEVICE=enp0s3
ONBOOT=yes

```

重启 network: `sudo service network restart` 失败,执行 `systemctl network.service` 看到 `Failed to start LSB`, 解决方法: 

1. 执行 `ip addr` 获取 MAC 地址: 08:00:27:c7:5f:bd
2. 修改/etc/sysconfig/network-scripts/下以ifcfg开头的网络链接文件中 HWADDR 为上一步中的 MAC 地址

配置三台机器互相 ssh 免密码登录, 生产私钥: `ssh-keygen -t rsa -P ''`

将所有机器上生成的 `/root/.ssh/id_rsa.pub` 内容都复制到各自的 `/root/.ssh/authorized_keys`

可以使用如下命令:

```
ssh-copy-id -i ~/.ssh/id_rsa.pub root@hserver2
```

### 所有机器安装 jdk 1.8

将下载下的压缩包解压到 /opt/java 下

添加环境变量到 `/etc/profile`

```
export JAVA_HOME=/opt/java/jdk1.8.0_161
export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib/
export PATH=$PATH:$JAVA_HOME/bin
```

```
source /etc/profile
java -version
```

### 所有机器安装 hadoop

将下载下的压缩包解压到 /opt/hadoop 下

```
tar -zxvf hadoop-2.8.3.tar.gz
```

创建如下目录:

```
mkdir  /root/hadoop  
mkdir  /root/hadoop/tmp  
mkdir  /root/hadoop/var  
mkdir  /root/hadoop/dfs  
mkdir  /root/hadoop/dfs/name  
mkdir  /root/hadoop/dfs/data 
```

修改 `/opt/hadoop/hadoop-2.8.3/etc/hadoop` 中配置文件:

1. `core-site.xml`

```
<configuration>
 <property>
        <name>hadoop.tmp.dir</name>
        <value>/root/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
</property>
   <property>
        <name>fs.default.name</name>
        <value>hdfs://hserver1:9000</value>
   </property>
</configuration>
```

2. 修改 `hadoop-env.sh`

```
export JAVA_HOME=/opt/java/jdk1.8.0_161
```

3. 修改 `hdfs-site.xml`

```
<property>
   <name>dfs.name.dir</name>
   <value>/root/hadoop/dfs/name</value>
   <description>Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.</description>
</property>

<property>
   <name>dfs.data.dir</name>
   <value>/root/hadoop/dfs/data</value>
   <description>Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.</description>
</property>

<property>
   <name>dfs.replication</name>
   <value>2</value>
</property>

<property>
      <name>dfs.permissions</name>
      <value>false</value>
      <description>need not permissions</description>
</property>
```

3. 复制 `mapred-site.xml.template` 到 `mapred-site.xml` 并修改

```
<property>
    <name>mapred.job.tracker</name>
    <value>hserver1:49001</value>
</property>

<property>
      <name>mapred.local.dir</name>
       <value>/root/hadoop/var</value>
</property>

<property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
</property>
```

4. 修改 `yarn-site.xml`

```
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hserver1</value>
   </property>

   <property>
        <description>The address of the applications manager interface in the RM.</description>
        <name>yarn.resourcemanager.address</name>
        <value>${yarn.resourcemanager.hostname}:8032</value>
   </property>

   <property>
        <description>The address of the scheduler interface.</description>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>${yarn.resourcemanager.hostname}:8030</value>
   </property>

   <property>
        <description>The http address of the RM web application.</description>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>${yarn.resourcemanager.hostname}:8088</value>
   </property>

   <property>
        <description>The https adddress of the RM web application.</description>
        <name>yarn.resourcemanager.webapp.https.address</name>
        <value>${yarn.resourcemanager.hostname}:8090</value>
   </property>

   <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>${yarn.resourcemanager.hostname}:8031</value>
   </property>

   <property>
        <description>The address of the RM admin interface.</description>
        <name>yarn.resourcemanager.admin.address</name>
        <value>${yarn.resourcemanager.hostname}:8033</value>
   </property>

   <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
   </property>

   <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
        <discription>每个节点可用内存,单位MB,默认8182MB</discription>
   </property>

   <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
   </property>

   <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1024</value>
</property>

   <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
```

5. 修改`slaves`文件

删除 localhost

```
hserver2  
hserver3 
```

### 启动 hadoop:

因为 hserver1 是 namenode,hserver2 和 hserver3 都是 datanode,所以只需要对 hserver1 进行初始化操作,也就是对 hdfs 进行格式化.
进入到 hserver1 这台机器的 `/opt/hadoop/hadoop-2.8.3/bin` 目录,执行命令:

```
 ./hadoop  namenode  -format
```

然后进入 `/opt/hadoop/hadoop-2.8.3/sbin` 执行命令:

```
./start-all.sh
```

### 测试 hadoop

先关闭 hserver1 防火墙:

```
systemctl   stop   firewalld.service
```

能访问如下链接则是正常的:

```
http://192.168.119.128:50070/
http://192.168.119.128:8088/
```

遇到一个问题 50070 端口可以正常访问, 8088 不能, 通过命令 `netstat -nap|grep 8088` 发现是有端口监听到,但是使用的是内网的ip, 所以需要改下 hserver1 的 `yarn-site.xml`

```
   <property>
        <description>The http address of the RM web application.</description>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>192.168.25.151:8088</value>
   </property>
```

### 安装 scala

下载 scala-2.12.2, [下载链接](https://downloads.lightbend.com/scala/2.12.2/scala-2.12.2.tgz)

解压到 `/opt/scala` 目录下,在 `/etc/profile` 下添加环境变量:

```
export SCALA_HOME=/opt/scala/scala-2.12.2 
export PATH = $PATH:$SCALA_HOME/bin
```

然后验证:

```
source   /etc/profile
scala -version
```

### 安装 spark

下载 spark-2.2.1-bin-hadoop2.7.tgz, 解压到 `/opt/spark` 目录下.

因为我们搭建的是基于hadoop集群的Spark集群,所以每个hadoop节点上我都安装了Spark,都需要按照下面的步骤做配置,启动的话只需要在Spark集群的Master机器上启动即可,我这里是在hserver1上启动.

配置环境变量到 `/etc/profile`:

```
export SPARK_HOME=/opt/spark/spark-2.2.1-bin-hadoop2.7 
export PATH = $PATH:$SPARK_HOME/bin
```

在 `/opt/spark/spark-2.2.1-bin-hadoop2.7/conf` 配置:

```
cp    spark-env.sh.template   spark-env.sh

# 添加内容
export SCALA_HOME=/opt/scala/scala-2.12.2  
export JAVA_HOME=/opt/java/jdk1.8.0_161  
export HADOOP_HOME=/opt/hadoop/hadoop-2.8.3
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop  
export SPARK_HOME=/opt/spark/spark-2.2.1-bin-hadoop2.7  
export SPARK_MASTER_IP=hserver1  
export SPARK_EXECUTOR_MEMORY=1G  

cp    slaves.template   slave
# 添加内容
hserver2  
hserver3 
```

### 测试 spark

 因为spark是依赖于hadoop提供的分布式文件系统的,所以在启动spark之前,先确保hadoop在正常运行.

 在hadoop正常运行的情况下,在hserver1（也就是hadoop的namenode,spark的marster节点）上执行命令:

```
cd   /opt/spark/spark-2.1.1-bin-hadoop2.7/sbin
./start-all.sh
```

启动成功会出现: Master, hserver2, hserver3 都启动,并能访问: `http://192.168.27.143:8080/`

成功的日志:
```
starting org.apache.spark.deploy.master.Master, logging to /opt/spark/spark-2.2.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.master.Master-1-hserver1.out
hserver2: starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/spark-2.2.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-hserver2.out
hserver3: starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/spark-2.2.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-hserver3.out
```


#### local模式运行

```
cd /opt/spark/spark-2.2.1-bin-hadoop2.7
./bin/spark-submit  --class  org.apache.spark.examples.SparkPi  --master local   examples/jars/spark-examples_2.11-2.2.1.jar  
```

#### 独立的Spark集群模式

不需要启动 hadoop, 进入 spark 目录, 直接 `./sbin/start-all.sh` 启动 spark 集群

```
./bin/spark-submit   --class   org.apache.spark.examples.SparkPi   --master   spark://192.168.119.128:7077   examples/jars/spark-examples_2.11-2.2.1.jar 
```

#### 使用Spark 集群+Hadoop集群

这种模式也叫On-Yarn模式,主要包括yarn-Client和yarn-Cluster两种模式.在这种模式下提交任务,需要先启动Hadoop集群,然后在启动Spark集群.

##### yarn-Client

```
./bin/spark-submit   --class  org.apache.spark.examples.SparkPi  --master  yarn-client    examples/jars/spark-examples_2.11-2.2.1.jar 
```

出现如下错误:

```
 Exceptionin thread "main" org.apache.spark.SparkException: Yarn applicationhas already ended! It might have been killed or unable to launch applicationmaster.  
```

解决方法: 修改 hadoop 的 `yarn-site.xml`

```
<property>   
   <name>yarn.nodemanager.vmem-check-enabled</name>   
   <value>false</value>   
</property>  
```

出现如下错误:

```
java.lang.IllegalArgumentException: Required executor memory (1024+384 MB) is above the max threshold (1024 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.
```

解决方法: 修改 hadop 的 `yarn-site.xml`

```
<property>  
        <name>yarn.scheduler.maximum-allocation-mb</name>  
        <value>2048</value>  
        <discription>每个节点可用内存,单位MB,默认8182MB</discription>  
   </property>  
   <property>  
       <name>yarn.nodemanager.vmem-pmem-ratio</name>  
        <value>2.1</value>  
   </property>  
  <property>  
       <name>yarn.nodemanager.resource.memory-mb</name>  
        <value>2048</value>  
</property> 
```

出现如下错误:

```
8/03/01 14:37:38 INFO hdfs.DFSClient: Exception in createBlockOutputStream
java.net.NoRouteToHostException: 没有到主机的路由
```

hserver1 能 ping 通 hserver2, hserver3 则关闭 hserver2, hserver3 的防火墙:

```
systemctl stop firewalld.service
```

##### yarn-Cluster

```
./bin/spark-submit  --class  org.apache.spark.examples.SparkPi  --master  yarn-cluster   examples/jars/spark-examples_2.11-2.2.1.jar 
```

注意,使用yarn-cluster模式计算,结果没有输出在控制台,结果写在了Hadoop集群的日志中,如何查看计算结果？注意到刚才的输出中有地址:



```
18/03/01 14:43:53 INFO yarn.Client:
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.119.129
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1519886591718
	 final status: SUCCEEDED
	 tracking URL: http://192.168.25.151:8088/proxy/application_1519886207949_0004/
	 user: root
18/03/01 14:43:53 INFO util.ShutdownHookManager: Shutdown hook called
18/03/01 14:43:53 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5117111d-5709-4d1a-a067-8fbf392fe2e0
```

直接用浏览器访问, 然后点击 Logs, 然后点击 stdout





