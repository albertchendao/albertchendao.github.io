---
layout: article
title: Spark 学习
tags: [spark,Java]
key: a13c2e74-4baf-43ce-b4d6-62513201a76c
---

单机启动 hadoop 并使用 Spark. 

### 依赖版本

1. hadoop   2.8.3
2. spark 2.2.1
3. jdk 1.8
4. scala 2.12.2

### 配置 ssh

首先生成私钥公钥 `ssh-keygen -t rsa`

通过如下命令设置 `ssh localhost` 免密码登录. 

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

如果遇到 `ssh: connect to host localhost port 22: Connection refused` 则选择系统偏好设置->选择共享->点击远程登录

### 安装 hadoop

下载 hadoop-2.8.3 并解压, 进入解压后的目录, 然后进入 `etc/hadoop`. 

修改 `hadoop-env.sh` 中的 `HADOOP_OPTS` 为:

```bash
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
```

修改 `core-site.xml` 中的 `<configuration></configuration>`:

```xml
<configuration>
  <property>
     <name>hadoop.tmp.dir</name>
<value>/usr/local/Cellar/hadoop/hdfs/tmp</value>
    <description>A base for other temporary directories.</description>
  </property>
  <property>
     <name>fs.default.name</name>
     <value>hdfs://localhost:8020</value>
  </property>
</configuration>
```

复制 `mapred-site.xml.template` 为 `mapred-site.xml`, 修改 `mapred-site.xml` 为:

```xml
<configuration>
      <property>
        <name>mapred.job.tracker</name>
        <value>localhost:8021</value>
      </property>
</configuration>
```

修改 `hdfs-site.xml`:

```xml
<configuration>
   <property>
     <name>dfs.replication</name>
     <value>1</value>
    </property>
</configuration>
```

添加 hadoop 环境变量, 我是使用 zsh, 所以修改 `~/.zshrc`:

```bash
export HADOOP_HOME=/Users/albertchen/tools/hadoop-2.8.3
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
```

格式化 HDFS: `hdfs namenode -format`

启动关闭 HDFS 命令:

```bash
start-dfs.sh          
stop-dfs.sh
```

启动成功后可以在 `http://localhost:50070` 查看到相关信息. 

启动关闭 YARN 命令:

```bash
./start-yarn.sh        
./stop-yarn.sh
```

启动成功后可以在 `http://localhost:8088` 查看到相关信息. 

### 安装 scala

```bash
brew install scala
```

### 安装 spark

下载 spark 并解压. 

在 `~/.zshrc` 添加:

```bash
export SPARK_HOME=/Users/albertchen/tools/spark-2.2.1
export PATH=$PATH:$SPARK_HOME/bin
```

启动 spark: `spark-shell`

使用单机local模式提交任务:

```bash
./bin/spark-submit  --class   org.apache.spark.examples.SparkPi  --master   local  examples/jars/spark-examples_2.11-2.2.1.jar
```

使用独立的Spark集群模式提交任务:

这种模式也就是Standalone模式, 使用独立的Spark集群模式提交任务, 需要先启动Spark集群, 但是不需要启动Hadoop集群. 启动Spark集群的方法是进入$SPARK_HOME/sbin目录下, 执行start-all.sh脚本, 启动成功后, 可以访问下面的地址看是否成功: `http://localhost:8080/`

```bash
./bin/spark-submit   --class   org.apache.spark.examples.SparkPi   --master   spark://192.168.27.143:7077   examples/jars/spark-examples_2.11-2.1.1.jar 
```
